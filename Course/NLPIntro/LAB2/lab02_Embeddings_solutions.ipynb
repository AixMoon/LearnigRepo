{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic word representations\n",
    "\n",
    "Semantic Word Representations are representations that are learned to capture the 'meaning' of a word based on the contexts it occurs in. These are low-dimensional vectors that contain some semantic properties. In this notebook we are going to build state-of-the art approaches to obtain semantic word representations using the `word2vec` modelling approach. We will use these vectors in some tasks to understand the utility of these representations. Later we will also learn how to use such already pre-trained representations (BERT) in those tasks.\n",
    "\n",
    "We begin by loading some of the libraries that are necessary for building our model. We are using [pytorch](https://pytorch.org/), an open source deep learning platform, as our backbone library in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/billchen/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "from tqdm import tqdm \n",
    "import codecs\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a Man',\n",
    "    'she Is a woman',\n",
    "    'london is, the capital of England',\n",
    "    'Berlin is ... the capital of germany',\n",
    "    'paris is the capital of france.',\n",
    "    'He will eat cake, pie, and/or brownies',\n",
    "    \"she didn't like the brownies\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse the pre-processing and vocabulary methods we have created in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'man'], ['she', 'is', 'a', 'woman'], ['london', 'is', ',', 'the', 'capital', 'of', 'england'], ['berlin', 'is', '...', 'the', 'capital', 'of', 'germany'], ['paris', 'is', 'the', 'capital', 'of', 'france', '.'], ['he', 'will', 'eat', 'cake', ',', 'pie', ',', 'and/or', 'brownies'], ['she', 'did', \"n't\", 'like', 'the', 'brownies']]\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "  tokenized_corpus = [] \n",
    "  for sentence in corpus:\n",
    "    sentence = \" \".join(word_tokenize(sentence))\n",
    "    tokenized_sentence = []\n",
    "    for token in sentence.split(' '):\n",
    "      token = token.lower()\n",
    "      tokenized_sentence.append(token)\n",
    "    tokenized_corpus.append(tokenized_sentence)\n",
    "  return tokenized_corpus\n",
    "\n",
    "def get_vocabulary(tokenized_corpus):\n",
    "  vocabulary = []\n",
    "  for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        vocabulary.append(token)\n",
    "  return set(vocabulary)\n",
    "\n",
    "tokenized_corpus = preprocess_corpus(corpus)\n",
    "print(tokenized_corpus)\n",
    "\n",
    "vocabulary = get_vocabulary(tokenized_corpus)\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Helper functions\n",
    "These are some of other common helper functions that are used for NLP models:\n",
    "\n",
    "`word2idx`: Maintains a dictionary of word and the corresponding index\n",
    "\n",
    "`idx2word`: Maintains a mapping from index to word\n",
    "\n",
    "Print the word2idx and idx2word, we will be using these in future exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'man': 0, 'she': 1, 'london': 2, 'capital': 3, 'germany': 4, 'paris': 5, 'and/or': 6, '...': 7, 'france': 8, ',': 9, 'queen': 10, 'cake': 11, 'the': 12, 'brownies': 13, 'berlin': 14, 'like': 15, 'will': 16, '.': 17, 'is': 18, 'king': 19, 'he': 20, 'pie': 21, 'england': 22, 'a': 23, 'of': 24, 'eat': 25, 'did': 26, \"n't\": 27, 'woman': 28}\n",
      "{0: 'man', 1: 'she', 2: 'london', 3: 'capital', 4: 'germany', 5: 'paris', 6: 'and/or', 7: '...', 8: 'france', 9: ',', 10: 'queen', 11: 'cake', 12: 'the', 13: 'brownies', 14: 'berlin', 15: 'like', 16: 'will', 17: '.', 18: 'is', 19: 'king', 20: 'he', 21: 'pie', 22: 'england', 23: 'a', 24: 'of', 25: 'eat', 26: 'did', 27: \"n't\", 28: 'woman'}\n"
     ]
    }
   ],
   "source": [
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
    "\n",
    "print(word2idx)\n",
    "print(idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look-up table\n",
    "\n",
    "This is a table that maps from an index to a one hot vector. It is a categorical variable binary representation where each row has one feature with a value of 1, and the other features with value 0. 1 in a particular column will tell the correct category for each example.\n",
    "\n",
    "**Q. Print one-hot vectors corresponding to the words 'the', 'he' and ''england'**\n",
    "\n",
    "*A. See code below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def look_up_table(word_idx):\n",
    "    x = torch.zeros(vocabulary_size).float()\n",
    "    x[word_idx] = 1.0\n",
    "    return x\n",
    "  \n",
    "# This is a one hot representation\n",
    "\n",
    "# Q. try printing it for word_idx = 1\n",
    "\n",
    "word_idx = word2idx['he']\n",
    "print(look_up_table(word_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting contexts and the focus word\n",
    "\n",
    "We will be building the **skip-gram** model. Given a corpus this model tries to use the current word (focus word) to predict its neighbors (its context)\n",
    "\n",
    "We first begin by obtaining the set of contexts and focus words.\n",
    "\n",
    "Let us say we have a sentence (represented as vocabulary indicies): [0, 2, 3, 6, 7].\n",
    "For every word in the sentence, we want to get the words which are window_size around it.\n",
    "So if window_size==2, for the word '0', we obtain: [[0, 2], [0, 3]]\n",
    "For the word '2', we obtain: [[2, 0], [2, 3], [2, 6]]\n",
    "For the word '3', we obtain: [[3, 0], [3, 2], [3, 6], [3, 7]]\n",
    "\n",
    "**Q. Print some of the index pairs and trace them back to their words.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 18]\n",
      " [20 23]\n",
      " [18 20]\n",
      " [18 23]\n",
      " [18 19]\n",
      " [23 20]\n",
      " [23 18]\n",
      " [23 19]\n",
      " [19 18]\n",
      " [19 23]\n",
      " [ 1 18]\n",
      " [ 1 23]\n",
      " [18  1]\n",
      " [18 23]\n",
      " [18 10]\n",
      " [23  1]\n",
      " [23 18]\n",
      " [23 10]\n",
      " [10 18]\n",
      " [10 23]\n",
      " [20 18]\n",
      " [20 23]\n",
      " [18 20]\n",
      " [18 23]\n",
      " [18  0]\n",
      " [23 20]\n",
      " [23 18]\n",
      " [23  0]\n",
      " [ 0 18]\n",
      " [ 0 23]\n",
      " [ 1 18]\n",
      " [ 1 23]\n",
      " [18  1]\n",
      " [18 23]\n",
      " [18 28]\n",
      " [23  1]\n",
      " [23 18]\n",
      " [23 28]\n",
      " [28 18]\n",
      " [28 23]\n",
      " [ 2 18]\n",
      " [ 2  9]\n",
      " [18  2]\n",
      " [18  9]\n",
      " [18 12]\n",
      " [ 9  2]\n",
      " [ 9 18]\n",
      " [ 9 12]\n",
      " [ 9  3]\n",
      " [12 18]\n",
      " [12  9]\n",
      " [12  3]\n",
      " [12 24]\n",
      " [ 3  9]\n",
      " [ 3 12]\n",
      " [ 3 24]\n",
      " [ 3 22]\n",
      " [24 12]\n",
      " [24  3]\n",
      " [24 22]\n",
      " [22  3]\n",
      " [22 24]\n",
      " [14 18]\n",
      " [14  7]\n",
      " [18 14]\n",
      " [18  7]\n",
      " [18 12]\n",
      " [ 7 14]\n",
      " [ 7 18]\n",
      " [ 7 12]\n",
      " [ 7  3]\n",
      " [12 18]\n",
      " [12  7]\n",
      " [12  3]\n",
      " [12 24]\n",
      " [ 3  7]\n",
      " [ 3 12]\n",
      " [ 3 24]\n",
      " [ 3  4]\n",
      " [24 12]\n",
      " [24  3]\n",
      " [24  4]\n",
      " [ 4  3]\n",
      " [ 4 24]\n",
      " [ 5 18]\n",
      " [ 5 12]\n",
      " [18  5]\n",
      " [18 12]\n",
      " [18  3]\n",
      " [12  5]\n",
      " [12 18]\n",
      " [12  3]\n",
      " [12 24]\n",
      " [ 3 18]\n",
      " [ 3 12]\n",
      " [ 3 24]\n",
      " [ 3  8]\n",
      " [24 12]\n",
      " [24  3]\n",
      " [24  8]\n",
      " [24 17]\n",
      " [ 8  3]\n",
      " [ 8 24]\n",
      " [ 8 17]\n",
      " [17 24]\n",
      " [17  8]\n",
      " [20 16]\n",
      " [20 25]\n",
      " [16 20]\n",
      " [16 25]\n",
      " [16 11]\n",
      " [25 20]\n",
      " [25 16]\n",
      " [25 11]\n",
      " [25  9]\n",
      " [11 16]\n",
      " [11 25]\n",
      " [11  9]\n",
      " [11 21]\n",
      " [ 9 25]\n",
      " [ 9 11]\n",
      " [ 9 21]\n",
      " [ 9  9]\n",
      " [21 11]\n",
      " [21  9]\n",
      " [21  9]\n",
      " [21  6]\n",
      " [ 9  9]\n",
      " [ 9 21]\n",
      " [ 9  6]\n",
      " [ 9 13]\n",
      " [ 6 21]\n",
      " [ 6  9]\n",
      " [ 6 13]\n",
      " [13  9]\n",
      " [13  6]\n",
      " [ 1 26]\n",
      " [ 1 27]\n",
      " [26  1]\n",
      " [26 27]\n",
      " [26 15]\n",
      " [27  1]\n",
      " [27 26]\n",
      " [27 15]\n",
      " [27 12]\n",
      " [15 26]\n",
      " [15 27]\n",
      " [15 12]\n",
      " [15 13]\n",
      " [12 27]\n",
      " [12 15]\n",
      " [12 13]\n",
      " [13 15]\n",
      " [13 12]]\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "idx_pairs = []\n",
    "\n",
    "# variables of interest: \n",
    "#   center_word_pos: center word position\n",
    "#   context_word_pos: context_word_position\n",
    "#   add sentence length as a constraint\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    \n",
    "    for center_word_pos in range(len(indices)):\n",
    "        \n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            \n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "                \n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs) # it will be useful to have this as numpy array\n",
    "\n",
    "print(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([9, 9]), array([18, 20]), array([10, 18]), array([25, 20]), array([20, 18])]\n",
      "\n",
      "[[',', ','], ['is', 'he'], ['queen', 'is'], ['eat', 'he'], ['he', 'is']]\n"
     ]
    }
   ],
   "source": [
    "# We'll sample 5 elements at random and trace these back to their word pairs\n",
    "from random import sample\n",
    "\n",
    "random_pairs = sample(list(idx_pairs), 5)\n",
    "print(random_pairs)\n",
    "\n",
    "tokens_from_idx_pairs = []\n",
    "for random_pair in random_pairs:\n",
    "    focus_word_idx, context_word_idx = random_pair[0], random_pair[1]\n",
    "    focus_word = idx2word[focus_word_idx]\n",
    "    context_word = idx2word[context_word_idx]\n",
    "    tokens_from_idx_pairs.append([focus_word, context_word])\n",
    "\n",
    "print()\n",
    "print(tokens_from_idx_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters and hyperparameters¶\n",
    "For our toy task, let us set the embedding dimensions to 5. Let us run the algorithm for 10 epochs (number of times the training algorithm sees all the training data). Let us choose the learning rate of 0.001. We have two parameter matrices $W_1$ and $W_2$ - the embedding matrix and the weight matrix.\n",
    "\n",
    "**Q. What are the dimensionalities of $W_1$ and $W_2$?** \n",
    "\n",
    "*A. shape(W1) = [embedding_dims x vocabulary_size]\n",
    "shape(W2) = [vocabulary_size x embedding_dims]*\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "embedding_dims = 5\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The two weight matrices:\n",
    "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "W2 = torch.randn(vocabulary_size, embedding_dims, requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training the model\n",
    "\n",
    "In the code below, we are going to compute the log probability of the correct context (target) given the word.\n",
    "\n",
    "**Q. Fill in the gaps.**\n",
    " \n",
    "**Q. Print the loss and see if the loss goes down.**\n",
    "\n",
    "\n",
    "**Q. What is the meaning of the negative log-likelihood loss (NLL) ?**\n",
    "\n",
    "The better the prediction the lower the NLL loss. Minimising the negative likelihood is the same as maximising the average log probability for the context window. We do it to be consistent with the rule of loss functions. Log is used for numerical stability. Likelihood refers to the chances that our outputs produce the target distribution.\n",
    "\n",
    "**Q. Which of the weight matrices will be extracted as word vectors ?**\n",
    "\n",
    "A. W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final epoch loss: 5.367302208800208\n",
      "\n",
      "Final epoch loss: 5.28241472991256\n",
      "\n",
      "Final epoch loss: 5.203510970554569\n",
      "\n",
      "Final epoch loss: 5.130393026033779\n",
      "\n",
      "Final epoch loss: 5.062788114919291\n",
      "\n",
      "Final epoch loss: 5.000346880067479\n",
      "\n",
      "Final epoch loss: 4.94265700779952\n",
      "\n",
      "Final epoch loss: 4.889269152632007\n",
      "\n",
      "Final epoch loss: 4.83972472029847\n",
      "\n",
      "Final epoch loss: 4.793579118205355\n",
      "\n",
      "Final epoch loss: 4.750420883491442\n",
      "\n",
      "Final epoch loss: 4.709881956314112\n",
      "\n",
      "Final epoch loss: 4.671639164933911\n",
      "\n",
      "Final epoch loss: 4.635414730805855\n",
      "\n",
      "Final epoch loss: 4.60097231028916\n",
      "\n",
      "Final epoch loss: 4.5681133490878265\n",
      "\n",
      "Final epoch loss: 4.536669080133562\n",
      "\n",
      "Final epoch loss: 4.506497740745544\n",
      "\n",
      "Final epoch loss: 4.477479602609362\n",
      "\n",
      "Final epoch loss: 4.44951267443694\n",
      "\n",
      "Final epoch loss: 4.4225097213472635\n",
      "\n",
      "Final epoch loss: 4.396395971248676\n",
      "\n",
      "Final epoch loss: 4.371106797224515\n",
      "\n",
      "Final epoch loss: 4.346585970420342\n",
      "\n",
      "Final epoch loss: 4.3227835669145955\n",
      "\n",
      "Final epoch loss: 4.299656014937859\n",
      "\n",
      "Final epoch loss: 4.277164452261739\n",
      "\n",
      "Final epoch loss: 4.255273464438203\n",
      "\n",
      "Final epoch loss: 4.233952454932324\n",
      "\n",
      "Final epoch loss: 4.2131726277339\n",
      "\n",
      "Final epoch loss: 4.1929083172377055\n",
      "\n",
      "Final epoch loss: 4.173135584050959\n",
      "\n",
      "Final epoch loss: 4.153832619066362\n",
      "\n",
      "Final epoch loss: 4.134979137352535\n",
      "\n",
      "Final epoch loss: 4.116556468722108\n",
      "\n",
      "Final epoch loss: 4.0985469462035535\n",
      "\n",
      "Final epoch loss: 4.08093417000461\n",
      "\n",
      "Final epoch loss: 4.063703182455781\n",
      "\n",
      "Final epoch loss: 4.046839514336028\n",
      "\n",
      "Final epoch loss: 4.030329612168399\n",
      "\n",
      "Final epoch loss: 4.014160376864594\n",
      "\n",
      "Final epoch loss: 3.9983198155056345\n",
      "\n",
      "Final epoch loss: 3.9827966496541904\n",
      "\n",
      "Final epoch loss: 3.967580020427704\n",
      "\n",
      "Final epoch loss: 3.952659672730929\n",
      "\n",
      "Final epoch loss: 3.9380257957941525\n",
      "\n",
      "Final epoch loss: 3.923668568010454\n",
      "\n",
      "Final epoch loss: 3.909579451982077\n",
      "\n",
      "Final epoch loss: 3.8957501402148953\n",
      "\n",
      "Final epoch loss: 3.882172465324402\n",
      "\n",
      "Final epoch loss: 3.8688382699892117\n",
      "\n",
      "Final epoch loss: 3.8557404705456326\n",
      "\n",
      "Final epoch loss: 3.842871751104082\n",
      "\n",
      "Final epoch loss: 3.830225091475945\n",
      "\n",
      "Final epoch loss: 3.8177943051635443\n",
      "\n",
      "Final epoch loss: 3.8055732776592306\n",
      "\n",
      "Final epoch loss: 3.7935560314686265\n",
      "\n",
      "Final epoch loss: 3.781736880927891\n",
      "\n",
      "Final epoch loss: 3.770109991748612\n",
      "\n",
      "Final epoch loss: 3.758670180648952\n",
      "\n",
      "Final epoch loss: 3.74741238897497\n",
      "\n",
      "Final epoch loss: 3.736331830551098\n",
      "\n",
      "Final epoch loss: 3.7254235403878346\n",
      "\n",
      "Final epoch loss: 3.7146829885321777\n",
      "\n",
      "Final epoch loss: 3.7041058114596774\n",
      "\n",
      "Final epoch loss: 3.6936878244598192\n",
      "\n",
      "Final epoch loss: 3.6834251679383314\n",
      "\n",
      "Final epoch loss: 3.6733137500750552\n",
      "\n",
      "Final epoch loss: 3.663349826614578\n",
      "\n",
      "Final epoch loss: 3.653529766318086\n",
      "\n",
      "Final epoch loss: 3.643850017677654\n",
      "\n",
      "Final epoch loss: 3.634307317145459\n",
      "\n",
      "Final epoch loss: 3.624898200685328\n",
      "\n",
      "Final epoch loss: 3.615619784051722\n",
      "\n",
      "Final epoch loss: 3.6064687487366913\n",
      "\n",
      "Final epoch loss: 3.5974420967040124\n",
      "\n",
      "Final epoch loss: 3.588537155033706\n",
      "\n",
      "Final epoch loss: 3.579751052639701\n",
      "\n",
      "Final epoch loss: 3.5710809493993785\n",
      "\n",
      "Final epoch loss: 3.5625244804791043\n",
      "\n",
      "Final epoch loss: 3.554079160287783\n",
      "\n",
      "Final epoch loss: 3.545742413440308\n",
      "\n",
      "Final epoch loss: 3.537511851106371\n",
      "\n",
      "Final epoch loss: 3.529385226887542\n",
      "\n",
      "Final epoch loss: 3.5213602719368873\n",
      "\n",
      "Final epoch loss: 3.513434664769606\n",
      "\n",
      "Final epoch loss: 3.505606474814477\n",
      "\n",
      "Final epoch loss: 3.497873863616547\n",
      "\n",
      "Final epoch loss: 3.490234584777386\n",
      "\n",
      "Final epoch loss: 3.4826867781676256\n",
      "\n",
      "Final epoch loss: 3.475228536438632\n",
      "\n",
      "Final epoch loss: 3.4678580590656827\n",
      "\n",
      "Final epoch loss: 3.4605734077366916\n",
      "\n",
      "Final epoch loss: 3.453372943710971\n",
      "\n",
      "Final epoch loss: 3.446255260473722\n",
      "\n",
      "Final epoch loss: 3.439218656583266\n",
      "\n",
      "Final epoch loss: 3.432261429049752\n",
      "\n",
      "Final epoch loss: 3.4253821597470866\n",
      "\n",
      "Final epoch loss: 3.418579203741891\n",
      "\n",
      "Final epoch loss: 3.411851073240305\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "  \n",
    "    loss_val = 0\n",
    "    \n",
    "    for data, target in idx_pairs:\n",
    "      \n",
    "        x = torch.Tensor(look_up_table(data))\n",
    "\n",
    "        # Q. what would y_true be? \n",
    "        y_true = torch.Tensor([target]).long()\n",
    "\n",
    "        # A. [index] of the target word\n",
    "        # \n",
    "        z1 = torch.matmul(W1, x) \n",
    "        \n",
    "        z2 = torch.matmul(W2, z1)\n",
    "        # Q. what is the above operation?\n",
    "        # A. matrix multiplication\n",
    " \n",
    "        # Let us obtain prediction over the vocabulary\n",
    "        log_softmax = F.log_softmax(z2, dim=0)\n",
    "                \n",
    "        loss = F.nll_loss(log_softmax.view(1,-1), y_true)\n",
    "        loss_val += loss.item()\n",
    "        \n",
    "        # propagate the error\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        # zero out gradient accumulation\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "\n",
    "    print(f'\\nFinal epoch loss: {loss_val/len(idx_pairs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Training with negative sampling\n",
    "\n",
    "Fill in the gaps in the code below for training the skipgram model with negative sampling. Train the model using the following corpus:  https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt. Use can use the GPU Colab Runtime. Submit the code and a short report (150 words) answering the following questions:  \n",
    "\n",
    "1. How did you preprocess your dataset and why?\n",
    "2. How did you pick negative examples? How many of them did you choose? Why did you decide to do so?\n",
    "3. How did you compute the final loss? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/train.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"train.txt\", \"r\") as f:\n",
    "    big_corpus = f.read()\n",
    "\n",
    "print(len(big_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "\n",
    "tokenized = word_tokenize(big_corpus.lower())\n",
    "vocabulary = set(tokenized)\n",
    "\n",
    "vocabulary_size = len(vocabulary)\n",
    "print(vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for Negative Sample:\n",
    "embedding_dims = 5\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# The two weight matrices:\n",
    "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "W2 = torch.randn(vocabulary_size, embedding_dims, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH0\n",
      "Loss at epoch 0: 1.9394938046862553\n",
      "EPOCH1\n",
      "EPOCH2\n",
      "EPOCH3\n",
      "EPOCH4\n",
      "EPOCH5\n",
      "EPOCH6\n",
      "EPOCH7\n",
      "EPOCH8\n",
      "EPOCH9\n",
      "EPOCH10\n",
      "Loss at epoch 10: 1.87205912743683\n",
      "EPOCH11\n",
      "EPOCH12\n",
      "EPOCH13\n",
      "EPOCH14\n",
      "EPOCH15\n",
      "EPOCH16\n",
      "EPOCH17\n",
      "EPOCH18\n",
      "EPOCH19\n",
      "EPOCH20\n",
      "Loss at epoch 20: 2.019749760337464\n",
      "EPOCH21\n",
      "EPOCH22\n",
      "EPOCH23\n",
      "EPOCH24\n",
      "EPOCH25\n",
      "EPOCH26\n",
      "EPOCH27\n",
      "EPOCH28\n",
      "EPOCH29\n",
      "EPOCH30\n",
      "Loss at epoch 30: 1.868371529958223\n",
      "EPOCH31\n",
      "EPOCH32\n",
      "EPOCH33\n",
      "EPOCH34\n",
      "EPOCH35\n",
      "EPOCH36\n",
      "EPOCH37\n",
      "EPOCH38\n",
      "EPOCH39\n",
      "EPOCH40\n",
      "Loss at epoch 40: 1.7476570551078041\n",
      "EPOCH41\n",
      "EPOCH42\n",
      "EPOCH43\n",
      "EPOCH44\n",
      "EPOCH45\n",
      "EPOCH46\n",
      "EPOCH47\n",
      "EPOCH48\n",
      "EPOCH49\n",
      "EPOCH50\n",
      "Loss at epoch 50: 1.5949684595810127\n",
      "EPOCH51\n",
      "EPOCH52\n",
      "EPOCH53\n",
      "EPOCH54\n",
      "EPOCH55\n",
      "EPOCH56\n",
      "EPOCH57\n",
      "EPOCH58\n",
      "EPOCH59\n",
      "EPOCH60\n",
      "Loss at epoch 60: 1.520374598586327\n",
      "EPOCH61\n",
      "EPOCH62\n",
      "EPOCH63\n",
      "EPOCH64\n",
      "EPOCH65\n",
      "EPOCH66\n",
      "EPOCH67\n",
      "EPOCH68\n",
      "EPOCH69\n",
      "EPOCH70\n",
      "Loss at epoch 70: 1.6091697681937125\n",
      "EPOCH71\n",
      "EPOCH72\n",
      "EPOCH73\n",
      "EPOCH74\n",
      "EPOCH75\n",
      "EPOCH76\n",
      "EPOCH77\n",
      "EPOCH78\n",
      "EPOCH79\n",
      "EPOCH80\n",
      "Loss at epoch 80: 1.553691165304029\n",
      "EPOCH81\n",
      "EPOCH82\n",
      "EPOCH83\n",
      "EPOCH84\n",
      "EPOCH85\n",
      "EPOCH86\n",
      "EPOCH87\n",
      "EPOCH88\n",
      "EPOCH89\n",
      "EPOCH90\n",
      "Loss at epoch 90: 1.5759592150132378\n",
      "EPOCH91\n",
      "EPOCH92\n",
      "EPOCH93\n",
      "EPOCH94\n",
      "EPOCH95\n",
      "EPOCH96\n",
      "EPOCH97\n",
      "EPOCH98\n",
      "EPOCH99\n"
     ]
    }
   ],
   "source": [
    "# The two weight matrices:\n",
    "W1 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "W2 = torch.randn(embedding_dims, vocabulary_size, requires_grad=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"EPOCH{}\".format(epoch))\n",
    "    epoch_loss = 0\n",
    "    for data, target in idx_pairs:\n",
    "        x_var = Variable(look_up_table(data)).float() \n",
    "        \n",
    "        y_pos = Variable(torch.from_numpy(np.array([target])).long())\n",
    "        y_pos_var = Variable(look_up_table(target)).float()\n",
    "\n",
    "        # TO DO: pick a negative sample\n",
    "        context_words = []\n",
    "        for one in idx_pairs:\n",
    "            if (one[0] == data):\n",
    "                context_words.append(one[1])\n",
    "\n",
    "        non_context_words = [i for i in range(vocabulary_size) if i not in set(context_words)]\n",
    "        # 5 negative samples\n",
    "        neg_indices = []\n",
    "        for i in range(5):\n",
    "            neg_indices.append(non_context_words[np.random.randint(len(non_context_words))])\n",
    "\n",
    "        neg_sample = Variable(torch.from_numpy(np.array(neg_indices)).long())\n",
    "        #TODO\n",
    "        # Now for test only\n",
    "        neg_sample =  Variable(torch.from_numpy(np.array([non_context_words[np.random.randint(len(non_context_words))]])).long())\n",
    "        y_neg_var = Variable(look_up_table(neg_sample)).float()\n",
    "        \n",
    "        #TO DO: use the look up table\n",
    "         \n",
    "        x_emb = torch.matmul(W1, x_var) \n",
    "        y_pos_emb = torch.matmul(W2, y_pos_var)\n",
    "        y_neg_emb = torch.matmul(W2, y_neg_var)\n",
    "        \n",
    "        # get positive sample score\n",
    "        pos_loss = F.logsigmoid(torch.matmul(x_emb, y_pos_emb))\n",
    "        \n",
    "        # get negsample score\n",
    "        neg_loss = F.logsigmoid(-1 * torch.matmul(x_emb, y_neg_emb))\n",
    "        \n",
    "        # TO DO: compute loss\n",
    "        loss = -(pos_loss + neg_loss)\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # propagate the error\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent\n",
    "        W1.data -= learning_rate * W1.grad.data\n",
    "        W2.data -= learning_rate * W2.grad.data\n",
    "\n",
    "        # zero out gradient accumulation\n",
    "        W1.grad.data.zero_()\n",
    "        W2.grad.data.zero_()\n",
    "        \n",
    "    if epoch % 10 == 0:    \n",
    "        print(f'Loss at epoch {epoch}: {epoch_loss/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab02_Embeddings_solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}