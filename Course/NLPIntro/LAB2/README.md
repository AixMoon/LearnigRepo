# LAB2

> Cited from Colab https://competitions.codalab.org/competitions/20970#learn_the_details-overview

## Overview

**SemEval-2020 Task 7: Assessing Humor in Edited News Headlines**

Join the task mailing list: [semeval-2020-task-7-all@googlegroups.com](https://groups.google.com/forum/#!forum/semeval-2020-task-7-all)

**Background and Significance:** Nearly all existing humor datasets are annotated to study whether a chunk of text is funny. However, it is interesting to study how short edits applied to a text can turn it from non-funny to funny. Such a dataset helps us focus on the humorous effects of atomic changes and the tipping point between regular and humorous text. The goal of our task is to determine how machines can understand humor generated by such short edits.

In addition, almost all humor datasets are annotated categorically, with the shared tasks being humor classification. However, humor occurs in various intensities, that is, certain jokes are much more funnier than others. A system's ability to assess the intensity of humor makes it useful in various applications, for example, humor generation where such a system can be used in a generate-and-test scheme to generate many potentially humorous texts and rank them in terms of funniness.

**Tasks:** In this competition, participants will estimate the funniness of news headlines that have been modified by humans using a **micro-edit** to make them funny. We define a headline micro-edit as any of the following replacements:

| **Replaced** | **Replacement** |
| ------------ | --------------- |
| entity       | noun            |
| noun         | noun            |
| verb         | verb            |

Each edited headline is scored by five judges, each of whom assigned a grade from one of the following:

| **Grade** | **Meaning**      |
| --------- | ---------------- |
| 0         | Not Funny        |
| 1         | Slightly Funny   |
| 2         | Moderately Funny |
| 3         | Funny            |

The ground truth funniness of each headline is the mean of its five funniness grades. Sample datapoints from the training set are shown below:

| **Original Headline**                                        | **Substitute** | **Grade** |
| ------------------------------------------------------------ | -------------- | --------- |
| Kushner to visit **Mexico** following latest Trump tirades   | **therapist**  | 2.8       |
| Hilllary Clinton Staffers Considered Campaign Slogan `Because It's Her **Turn**' | **fault**      | 2.8       |
| The Latest: BBC cuts **ties** with Myanmar TV station        | **pies**       | 1.8       |
| Oklahoma isn't **working**. Can anyone fix this failing American state? | **okay**       | 0.0       |
| 4 **soldiers** killed in Nagorno-Karabakh fighting: Officials | **rabbits**    | 0.0       |

 

There will be two sub-tasks that you can participate in:

1. **Regression:** Given the original and the edited headline, the participant is required to predict the mean funniness of the edited headline.
2. **Predict the funnier of the two edited headlines**: Given the original headline and two edited versions, the participant has to predict which edited version is the funnier of the two.

This dataset was introduced in the following publication:

- Nabil Hossain, John Krumm and Michael Gamon. ["President Vows to Cut Taxes Hair": Dataset and Analysis of Creative Text Editing for Humorous Headlines.](https://www.aclweb.org/anthology/N19-1012) 2019. In *NAACL.*

## Evaluation

Note**:** Evaluations on Test Set will happen during Jan 10-31, 2020 according to SemEval rules.

#### **Sub-Task 1: Regression.**

Systems will be ranked using the [Root Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) on the overall test set. The file uploaded for evaluation must be a **zip file** containing a csv file called "task-1-output.csv" having two columns in the following order:

- id: the ID of the edited headline as provided in the dataset
- pred: the estimated funniness for the headline, a real number in the 0-3 funniness interval.

Please include the column headers and name them exactly as above, and in the order mentioned. A sample output (for the baseline system) can be found [here](https://github.com/n-hossain/semeval-2020-task-7-humicroedit/blob/master/output_dev_baseline/task-1-output.csv).

We will additionally report RMSE by taking the N% most funny headlines and N% least funny headlines in the test set, for N âˆˆ {10,20,30,40}. For example, N=30 implies sorting the test set from most funny to least funny and using the top 30% and the bottom 30% of this sorted data, for a total of 60% of the test set, to calculate the RMSE. These are meant to be additional evaluation metrics, and they will not be used to rank systems.

 

#### **Sub-Task 2: Predict funnier of the two edited versions of an original headline.**

Systems will be ranked based on the accuracy in predicting the funnier of the two edited versions of the same original headline according to the ground truth mean funniness on the test set. System outputs will be ignored for cases where the two edited headlines have the same ground truth mean funniness.

The file uploaded for evaluation must be a **zip file** containing a csv file called "task-2-output.csv" having two columns in the following order:

- id: the ID of the two edited headlines separated by "-" as provided in the dataset.
- pred: the edited headline which is predicted the funnier of the two.
  - 1 implies headline 1 is predicted funnier.
  - 2 implies headline 2 is predicted funnier.

Please include the column headers and name them exactly as above, and in the order mentioned. A sample output (for the baseline system) can be found [here](https://github.com/n-hossain/semeval-2020-task-7-humicroedit/blob/master/output_dev_baseline/task-2-output.csv).

We will also report another evaluation metric called the reward, calculated as follows:

- For a correct prediction, the pair-wise reward is the positive difference between the mean grades of the two headlines.
- For a wrong prediction, the pair-wise reward is the negative difference between the mean grades of the two headlines.

Overall reward is the mean of these pair-wise rewards (here we also ignore cases where the two edited headlines have the same ground truth mean funniness). This metric will not be used to rank teams.

## Datasets

### Datasets

The data for the two tasks can be found [here](http://cs.rochester.edu/u/nhossain/humicroedit.html). 

There are about 5,000 original headlines, each having 3 edited versions (a total of about 15,000 headlines). Data has been randomly partitioned into the following subsets:

| **Subset** | **Allocation** | **Data Released** | **Labels Released** |
| ---------- | -------------- | ----------------- | ------------------- |
| Train      | 64%            | June 1, 2019      | June 1, 2019        |
| Dev        | 16%            | June 1, 2019      | Feb 20, 2019        |
| Test       | 20%            | Feb 20, 2019      | TBD                 |

**Note**: Test set evaluations happen during Jan 10-31, 2020 according to SemEval rules.

Data has been paritioned based on the original headline, that is, all edited versions of the same original headline are in the same and exactly one partition.

For sub-task 1, each datapoint contains the following fields:

| **Field** | **Example**                                                | **Remarks**                                                  |
| --------- | ---------------------------------------------------------- | ------------------------------------------------------------ |
| id        | 1183                                                       | Unique identifier of an edited headline.                     |
| original  | Kushner to visit <Mexico/> following latest trump tirades. | The replaced word(s) identified with the </> tag.            |
| edit      | therapist                                                  |                                                              |
| grades    | 33332                                                      | Concatenated into a string. There can be 10 or 15 grades, e.g. when 2 or 3 editors made the same edit. |
| meanGrade | 2.8                                                        | The mean of all the judges' grades.                          |

For sub-task 2, the fields are: id, original1, edit1, grades1, meanGrade1, original2, edit2, grades2, meanGrade2, label. Further details:

- id: id of edited headline 1 followed by "-" followed by id of edited headline 2.
- "label" can take the following values:
  - 0 - both headlines have the same funniness (these are ignored during evaluation).
  - 1 - edited headline 1 is funnier.
  - 2 - edited headline 2 is funnier.


## Reference

[https://medium.com/@cselig/assessing-the-funniness-of-edited-news-headlines-3ec03056f29a](https://medium.com/@cselig/assessing-the-funniness-of-edited-news-headlines-3ec03056f29a)