# ML Assignment 7

> 10185101210 陈俊潼

## 5.1 - 5.7

C, A, BD, A, B, B, A

## 5.8

对于逻辑回归算法和支持向量机算法，都是简单的单个学习器，可以先比较他们的异同。

其相同之处为：逻辑回归和支持向量机都是分类算法，他们都是线性分类模型，而且都是线性分类模型。同时他们也都是监督学习方法，通常都是弱分类器，单个使用的效果不是很好。

不同之处有：

1. 损失函数不同。逻辑回归的损失函数为：
$$
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m}\left(y_{i} \log h_{\theta}\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-h_{\theta}\left(x_{i}\right)\right)\right)
$$
而支持向量机的损失函数为：
$$
L(w, b, \alpha)=\frac{1}{2}\|w\|^{2}-\sum_{i=1}^{n} \alpha_{i}\left(y_{i}\left(w^{T} x_{i}+b\right)-1\right)
$$
因为逻辑回归基于极大似然法估计参数的值，来取得最优；而支持向量机的原理则是基于集合间隔最大化，把最大集合间隔面作为最优的分类面。

2. 支持向量机只考虑分类面附近的点，尝试使间隔最大化，而逻辑回归需要考虑所有的点。
3. 在解决非线性的分类问题时，支持向量机通常会采用核函数的方法，但逻辑回归通常不采用。
4. 支持向量机的损失函数带有正则惩罚项，能够做到结构风险最小化。而逻辑回归算法则需要额外在损失函数上加上正则项来实现。

而 `AdaBoosting`  是一种集成学习的算法，是 `Boosting` 算法的一个分支。通过基于调整样本的分布来重采样，多次训练下一个基学习器来达到更好的学习效果，将弱学习器提升为强学习器。支持向量机和逻辑回归算法都可以作为 `AdaBoosting` 算法的基学习器。